import os
import streamlit as st

from search_docs import buscar_documentos_relevantes
from generate_response import generar_respuesta_con_llm
from generate_embeddings import create_vectorstore, load_documents

st.set_page_config(page_title="ğŸ§  LLM Categorizer Demo", layout="centered")

st.title("ğŸ§± LLM Categorizer Demo")
st.write("Esta demo utiliza Gemini para generar respuestas a partir de documentos cargados.")

VECTOR_DIR = "vectorstore"

# Si no existe el Ã­ndice FAISS, lo genera automÃ¡ticamente
if not os.path.exists(VECTOR_DIR):
    st.warning("âš ï¸ No se encontrÃ³ el Ã­ndice FAISS. GenerÃ¡ndolo automÃ¡ticamente...")
    docs = load_documents()
    if docs:
        create_vectorstore(docs)
        st.success("âœ… Ãndice generado con Ã©xito.")
    else:
        st.error("âŒ No se encontraron documentos en /data/documentos_raw/.")

# Input del usuario
pregunta = st.text_input("ğŸ” IngresÃ¡ tu consulta:")

if pregunta:
    with st.spinner("ğŸ” Buscando documentos relevantes..."):
        fragmentos = buscar_documentos_relevantes(pregunta)

    st.subheader("ğŸ“„ Fragmentos relevantes encontrados:")
    for i, doc in enumerate(fragmentos, 1):
        st.markdown(f"**Fragmento #{i}:**")
        st.info(doc.page_content)

    with st.spinner("âœï¸ Generando respuesta..."):
        respuesta = generar_respuesta_con_llm(pregunta, fragmentos, modelo="gemini")

    st.subheader("ğŸ“Œ Respuesta generada:")
    st.success(respuesta)
