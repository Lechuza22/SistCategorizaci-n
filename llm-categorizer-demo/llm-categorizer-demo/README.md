# ğŸ§± LLM Categorizer Demo (Gemini + Hugging Face)

Este proyecto es una **demo funcional** de un sistema inteligente que procesa preguntas de usuarios, **busca documentos relevantes** y **genera respuestas** usando un modelo de lenguaje (Gemini Pro) y bÃºsqueda semÃ¡ntica con embeddings de Hugging Face.

---

## ğŸš€ Â¿QuÃ© hace esta aplicaciÃ³n?

1. Carga documentos en texto o PDF.
2. Fragmenta y vectoriza el contenido usando **FAISS** y **Hugging Face embeddings**.
3. Acepta preguntas de usuarios.
4. Recupera los fragmentos mÃ¡s relevantes con bÃºsqueda semÃ¡ntica.
5. Construye un prompt con contexto y lo envÃ­a al **modelo Gemini**.
6. Devuelve una respuesta clara y contextualizada al usuario.

---

## ğŸ§° TecnologÃ­as utilizadas

- [Python](https://www.python.org/)
- [Streamlit](https://streamlit.io/)
- [LangChain](https://www.langchain.com/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Hugging Face Transformers](https://huggingface.co/)
- [Google Generative AI (Gemini)](https://makersuite.google.com/)
- [dotenv](https://pypi.org/project/python-dotenv/)

---

## ğŸ“ Estructura del proyecto

```
llm-categorizer-demo/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                # Interfaz Streamlit
â”‚   â”œâ”€â”€ config.py              # Clave API
â”‚   â”œâ”€â”€ generate_embeddings.py # Carga y vectorizaciÃ³n de documentos
â”‚   â”œâ”€â”€ search_documents.py    # BÃºsqueda semÃ¡ntica con FAISS
â”‚   â””â”€â”€ generate_response.py   # GeneraciÃ³n de respuesta con Gemini
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documentos_raw/        # Tus archivos .txt o .pdf
â”œâ”€â”€ vectorstore/               # Ãndice FAISS generado
â”œâ”€â”€ .env.example               # Archivo de ejemplo para configuraciÃ³n
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ InstalaciÃ³n y ejecuciÃ³n

### 1. ClonÃ¡ el repositorio

```bash
git clone https://github.com/tu-usuario/llm-categorizer-demo.git
cd llm-categorizer-demo
```

### 2. InstalÃ¡ dependencias

```bash
pip install -r requirements.txt
```

### 3. ConfigurÃ¡ la clave de Gemini

Crea un archivo `.env` en la raÃ­z con:

```env
GEMINI_API_KEY=tu_clave_de_gemini
```

### 4. ColocÃ¡ documentos en la carpeta

```bash
/data/documentos_raw/
```

PodÃ©s usar `.txt` o `.pdf`.

### 5. GenerÃ¡ los embeddings

```bash
python app/generate_embeddings.py
```

### 6. EjecutÃ¡ la aplicaciÃ³n

```bash
streamlit run app/main.py
```

---

## ğŸ§ª Ejemplo de uso

1. SubÃ­ un archivo `.txt` con contenido tÃ©cnico.
2. HacÃ© una consulta desde la interfaz.
3. El sistema buscarÃ¡ los fragmentos mÃ¡s relevantes.
4. El modelo Gemini generarÃ¡ una respuesta basada en los documentos.

---

## ğŸ§  Casos de uso

- Chatbots internos basados en documentaciÃ³n de empresa
- Asistentes tÃ©cnicos entrenados con manuales
- Sistemas de ayuda en sectores como:
  - ConstrucciÃ³n
  - EducaciÃ³n
  - Soporte de software

---

## ğŸ“Œ CrÃ©ditos

Desarrollado por [JerÃ³nimo MartÃ­nez](https://www.linkedin.com/in/jeronimo-martinez/), Data Scientist especializado en soluciones aplicadas con IA.

---

## ğŸ“„ Licencia

Este proyecto se publica bajo la licencia MIT.
